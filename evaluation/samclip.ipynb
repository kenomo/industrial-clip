{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../CLIP\")\n",
    "sys.path.append(\"..\")\n",
    "from industrial_clip.evaluation import Evaluation\n",
    "\n",
    "from segment_anything import build_sam, sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "from evaluation.utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sam results\n",
    "def plot_sam_result(masks, image):\n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    detections = sv.Detections.from_sam(sam_result=masks)\n",
    "    image = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    annotated_image = mask_annotator.annotate(scene=image, detections=detections)\n",
    "\n",
    "    sv.plot_images_grid(\n",
    "        images=[image, annotated_image],\n",
    "        grid_size=(1, 2),\n",
    "        titles=['Source Image', 'Segmented Image']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the individual segments\n",
    "def visualize_segments(segments, result, prompt_idx=0):\n",
    "    cols = 10\n",
    "    rows = math.ceil(len(segments) / cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, img in enumerate(segments):\n",
    "        axs[i].imshow(img)\n",
    "        #axs[i].set_title(round(result[i][prompt_idx].item(), 3), fontsize=30)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    for ax in axs[i+1:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot language-guided segmentation results\n",
    "def plot_results(axs, res, pred_prompt_idx, threshold, resize=False, image_shape=None):\n",
    "    cmap = mcolors.ListedColormap(['none', 'red'])\n",
    "\n",
    "    indices = get_predictions_above_threshold(res, pred_prompt_idx, threshold=threshold)\n",
    "    for idx in indices:\n",
    "        # get mask\n",
    "        mask = masks[idx][\"segmentation\"]\n",
    "\n",
    "        if resize:\n",
    "            mask = cv2.resize(mask.astype(float), (image_shape[1], image_shape[0]))\n",
    "\n",
    "        # overlay the mask\n",
    "        axs.imshow(mask, cmap=cmap, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize language-guided segmentation results in one plot\n",
    "def visualize(res_zeroshotclip, res_iclip, pred_prompt_idx, threshold, image, resize=False, vertical=False, name=None):\n",
    "    if vertical:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(7.8, 17.5))\n",
    "    else:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(31.5, 6.3))\n",
    "    \n",
    "    font = FontProperties()\n",
    "    font.set_name('Arial')\n",
    "\n",
    "    axs[0].axis('off')\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Input\", fontproperties=font, fontweight='bold', fontsize=30, pad=9)\n",
    "\n",
    "    axs[1].axis('off')\n",
    "    axs[1].imshow(image)\n",
    "    axs[1].set_title(\"Zero-shot CLIP\", fontproperties=font, fontweight='bold', fontsize=30, pad=9)\n",
    "    plot_results(axs[1], res_zeroshotclip, pred_prompt_idx, threshold, resize=resize, image_shape=image.shape)\n",
    "\n",
    "    axs[2].axis('off')\n",
    "    axs[2].imshow(image)\n",
    "    axs[2].set_title(\"Results\", fontproperties=font, fontweight='bold', fontsize=30, pad=9)\n",
    "    plot_results(axs[2], res_iclip, pred_prompt_idx, threshold, resize=resize, image_shape=image.shape)\n",
    "\n",
    "    if vertical:\n",
    "        fig.tight_layout(pad=0, h_pad=1.0)\n",
    "        fig.subplots_adjust(bottom=0, top=0.97, left=0.0, right=1.0)\n",
    "    else:\n",
    "        fig.tight_layout(pad=0, h_pad=0.0, w_pad=0.05)\n",
    "        fig.subplots_adjust(bottom=-0.10, top=0.90, left=-0.01, right=1.01)\n",
    "\n",
    "    if name is not None:\n",
    "        fig.savefig(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval SAM + CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model_type = \"vit_h\"\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "\n",
    "sam_model = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint)\n",
    "sam_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(clip_model, clip_model_dir, clip_epoch, image_path, prompts, segments_with_background=False):\n",
    "    full_res_image, resized_image = load_and_resize_image(image_path, sam_image_size)\n",
    "\n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        model=sam_model, \n",
    "        points_per_side=sam_points_per_side, \n",
    "        pred_iou_thresh=sam_predicted_iou_threshold, \n",
    "        stability_score_thresh=sam_stability_score_thresh, \n",
    "        stability_score_offset=sam_stability_score_offset,\n",
    "        box_nms_thresh=sam_box_nms_thresh, \n",
    "        min_mask_region_area=sam_min_mask_region_area\n",
    "    )\n",
    "\n",
    "    # generate masks\n",
    "    masks = mask_generator.generate(resized_image)\n",
    "    # remove masks that are too small\n",
    "    masks = [mask for mask in masks if mask[\"area\"] > sam_min_mask_size]\n",
    "\n",
    "    # extract segments\n",
    "    segments = []\n",
    "    new_masks = []\n",
    "    for mask in masks:\n",
    "        segmented_image = segment_image(resized_image, mask[\"segmentation\"])\n",
    "        bbox = convert_box_xyxy_dilate_square(mask[\"bbox\"], image_size=resized_image.shape[0:2], make_squared=False, dilation=dilation)\n",
    "        if not segments_with_background:\n",
    "            segment = segmented_image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "        else:\n",
    "            segment = resized_image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "\n",
    "        if segment.shape[0] == 0 or segment.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        if segment.shape[0] + 10 >= resized_image.shape[0] or segment.shape[1] + 10 >= resized_image.shape[1]:\n",
    "            continue\n",
    "        \n",
    "        new_masks.append(mask)\n",
    "        segments.append(segment)\n",
    "\n",
    "    masks = new_masks\n",
    "\n",
    "    cfg_list = [\n",
    "        \"TRAINER.ZSCLIP.PROMPT_TEMPLATE\", \"a photo of an industrial product {}\",\n",
    "    ]\n",
    "    zeroshotclip = Evaluation(\"ZeroshotCLIP\", cfg_list, prompts)\n",
    "    zeroshotclip.build()\n",
    "\n",
    "    cfg_list = [\n",
    "        \"TRAINER.COOP.CTX_INIT\", \"X X X X a photo of an industrial product\",\n",
    "        \"TRAINER.COOP.CLASS_TOKEN_POSITION\", \"end\"\n",
    "    ]\n",
    "    iclip = Evaluation(clip_model, cfg_list, prompts, model_dir=clip_model_dir, epoch=clip_epoch)\n",
    "    iclip.build()\n",
    "\n",
    "    image_tensors = numpy_to_tensor(segments, image_size=zeroshotclip.visual_resolution)\n",
    "    prompt_idx = list(range(len(prompts)))\n",
    "\n",
    "    res_zeroshotclip = zeroshotclip.forward(image_tensors, prompt_idx)[0]\n",
    "    res_iclip = iclip.forward(image_tensors, prompt_idx)[0]\n",
    "\n",
    "    return (full_res_image, resized_image, res_zeroshotclip, res_iclip, masks, segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SAM parameters\n",
    "\n",
    "sam_image_size = 1024\n",
    "sam_predicted_iou_threshold = 0.90\n",
    "sam_stability_score_thresh = 0.90\n",
    "sam_stability_score_offset = 1.0\n",
    "sam_box_nms_thresh = 0.6\n",
    "sam_min_mask_region_area = 700\n",
    "sam_min_mask_size = 700\n",
    "sam_points_per_side = 160\n",
    "dilation = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# set the path to the image, prompt, model, path to model, and epoch (=40)\n",
    "\n",
    "image_path = \"<<<path to image>>>\"\n",
    "prompts = [\n",
    "    \"<<<add prompt>>>\",\n",
    "    \"\"\n",
    "]\n",
    "full_res_image, resized_image, res_zeroshotclip, res_iclip, masks, segments = eval(\n",
    "    \"CoOpIATA\",\n",
    "    \"<<<path to model>>>\",\n",
    "    40,\n",
    "    image_path, \n",
    "    prompts, \n",
    "    segments_with_background=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sam_result(masks, resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segments(segments, res_iclip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prompt_idx = 0\n",
    "threshold = 0.90\n",
    "print(\"Prompt: %s\" % prompts[pred_prompt_idx])\n",
    "visualize(res_zeroshotclip, res_iclip, pred_prompt_idx, threshold, resized_image, vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "industrial-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
